{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f53c42d",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "\n",
    "1 = Simple linear regression only has 1 predictor variable. Multiple Linear Regression has multiple predictor variables. The benefits of Multiple Linear Regression over Simple Linear Regression is that it higher potential of accuracy due to taking into account of more predictor variables towards the outcome variable. Furthermore, the output variable can be analyzed in a more nuanced and sophisticated manner by evaluating the effect of each individual predictor variable on the output variable\n",
    "\n",
    "\n",
    "2 = An indicator variable is a special variable that either takes the values of 0 or 1 and it is typically referred to as a \"dummy variable\". A continuous variable is a variable that can take any real number.\n",
    "\n",
    "The simple linear form for a continous variable would be (*y=beta^0+beta^1x*) where x is any real number for the slope that results in a change in the output variable of y. \n",
    "\n",
    "The simple linear form for an indicator variable would be (*y=beta^0+beta^1D*) where y=beta^0 when D=0 for beta^1 and where y=beta^1 when D=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3 = The behavior change in the model when it is transitioned from a Simple Linear Regression to a Multiple Linear Regression model is the introduction of two parallel lines on the model compared to one on the Simple linear regression model. There are two lines as the indicator dummy variable splits the values of the continous variable into groups of 0 or 1. It is important to note that the values of the continous variable do not change, however, they are put into different groups based upon meeting the group definition criteria for the indicator value of 0 or 1. \n",
    "\n",
    "This can be shown on linear regression equations. The simple linear equation (*y=beta^0+beta^1x*) has beta^1 being the continous predictor variable as the slope, where a change in x for the continous variable (alongside the intercept of beta^0) results in a change of the outcome variable of y. \n",
    "\n",
    "There are two multiple linear equation ((*y=beta^0+beta^1x+beta^2D*) if dummy variable = 1 & (*y=beta^0+beta^1x*) if dummy variable = 0). Since the dummy variable (D) is either 0 or 1, beta^2 simply indicates whether the indicator variable is 1 or 0. Contextually, both equations thus are representing the linear regression of continuous variables that are placed in D=0 or D=1 (e.g of this would be, for example, D=1 is for woman and D=0 is for men when the continous variable is hours worked per week. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4 = The effect of adding an interaction between a continuous and an indicator variable in Multiple Linear Regression models results in the slope between the two linear models (seperated by the indicator variable) to be different and thus the two linear lines are no longer parallel. \n",
    "\n",
    "There are two equations here:\n",
    "(*y=beta^0+beta^1x+beta^2D+beta^3(xD)*) when D=1\n",
    "\n",
    "& (*y=beta^0+beta^1x) when D=0\n",
    "\n",
    "Regarding the indicator group of D=1, the introduction of beta^3 into the equations from question 3 directly changes the slope of the continous variable to be proportional to the indicator variable. Thus, the linear regression lines of the continous variables for D=0 and D=1 are now proportional to their indicator group (which is a key nuance that people use multiple linear regression for).\n",
    "\n",
    "5 = If we had a Multiple Linear Regression Model based upon only indicator variables (for non-binary categorical data) there would be multiple indicator variables, where each categorical input has at least 1 indicator variable where D=1 is true for that input (and D=0 for the rest). It is referred to as dummy coding. \n",
    "\n",
    "e.g, assume we have 3 categorical inputs (A, B, & C)\n",
    "(*y=beta^0+beta^1D^1+beta^2D^2+beta^3D^3*)\n",
    "\n",
    "for Beta 1, A=1, B=0, C=0\n",
    "\n",
    "for Beta 2, A=0, B=1, C=0\n",
    "\n",
    "for Beta 3, A=0, B=0, C=1\n",
    "\n",
    "Thus for example, if we only look at category A, the resulting equation would be:\n",
    "\n",
    "(*y=beta^0+beta^1D^1*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f6745",
   "metadata": {},
   "source": [
    "# CoPilot Summary (Question 1)\n",
    "\n",
    "Of course! Here's a summary of our conversation:\n",
    "\n",
    "### Summary of Our Discussion\n",
    "\n",
    "1. **Simple Linear Regression vs. Multiple Linear Regression**:\n",
    "   - **Simple Linear Regression**: Models the relationship between a single continuous predictor and the response variable.\n",
    "     - **Equation**: $$ y = \\beta_0 + \\beta_1 x + \\epsilon $$\n",
    "   - **Multiple Linear Regression**: Extends to multiple predictors, including continuous and indicator variables.\n",
    "     - **Equation**: $$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_n x_n + \\epsilon $$\n",
    "   - **Benefit**: Multiple Linear Regression can provide more accurate predictions by considering multiple factors.\n",
    "\n",
    "2. **Continuous Variable vs. Indicator Variable in Simple Linear Regression**:\n",
    "   - **Continuous Variable**: Represents a range of values.\n",
    "     - **Equation**: $$ y = \\beta_0 + \\beta_1 x + \\epsilon $$\n",
    "   - **Indicator Variable**: Binary variable (0 or 1) representing categories.\n",
    "     - **Equation**: $$ y = \\beta_0 + \\beta_1 D + \\epsilon $$\n",
    "   - **Interpretation**: \\( \\beta_0 \\) is the intercept for \\( D = 0 \\), and \\( \\beta_0 + \\beta_1 \\) is the intercept for \\( D = 1 \\).\n",
    "\n",
    "3. **Introducing an Indicator Variable in Multiple Linear Regression**:\n",
    "   - **Equation**: $$ y = \\beta_0 + \\beta_1 x + \\beta_2 D + \\epsilon $$\n",
    "   - **Behavior**: Adjusts the intercept based on the indicator variable, allowing the model to account for categorical differences.\n",
    "\n",
    "4. **Interaction Between Continuous and Indicator Variable**:\n",
    "   - **Equation**: $$ y = \\beta_0 + \\beta_1 x + \\beta_2 D + \\beta_3 (x \\cdot D) + \\epsilon $$\n",
    "   - **Effect**: \\( \\beta_3 \\) adjusts the slope of \\( x \\) based on \\( D \\), allowing different relationships between \\( x \\) and \\( y \\) for different groups.\n",
    "\n",
    "5. **Multiple Linear Regression with Indicator Variables from a Non-Binary Categorical Variable**:\n",
    "   - **One-Hot Encoding**: Converts a categorical variable into multiple binary variables.\n",
    "   - **Example**: Education level with categories \"High School,\" \"Bachelor's,\" and \"Master's.\"\n",
    "     - **Equation**: $$ y = \\beta_0 + \\beta_1 D_1 + \\beta_2 D_2 + \\beta_3 D_3 + \\epsilon $$\n",
    "   - **Behavior**: Captures differences in the response variable based on the categories.\n",
    "\n",
    "### Key Points\n",
    "- **One-Hot Encoding**: Method to handle non-binary categorical variables by creating binary indicator variables.\n",
    "- **Interaction Terms**: Allow the effect of one predictor to depend on the level of another predictor, providing a more flexible model.\n",
    "- **Model Interpretation**: Coefficients of indicator variables represent differences in the response variable compared to a reference category.\n",
    "\n",
    "I hope this summary helps with your homework assignment! If you need any more details or have further questions, feel free to ask. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f680250",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "\n",
    "Outcome Variable: Advertising Effectiveness (sales)\n",
    "\n",
    "Predictor Variables:TV Advertisement Spending & sales, Online Advertisement Spending & sales\n",
    "\n",
    "**Continuous Variable:**\n",
    "\n",
    "- W/o interaction: $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad Spend}) + \\beta_2 (\\text{Online Ad Spend})$$\n",
    "\n",
    "\n",
    "- With interaction: $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad Spend}) + \\beta_2 (\\text{Online Ad Spend}) + \\beta_3 (\\text{TV Ad Spend} \\times \\text{Online Ad Spend})$$\n",
    "\n",
    "\n",
    "TV_ad_spent = sales * money spent/1,000\n",
    "\n",
    "Online_ad_spent = sales * money spent/1,000\n",
    "\n",
    "The non-interaction equation assumes TV spending & Online spending are seperate, thus it is simply y=a(x)+b(x). There is an assumption that TV spending & sales gained does not affect the online spending on ads & sales gained from online spending (and vice versa).\n",
    "\n",
    "The interaction equation assumes TV spending & Online spending are dependent on each other, thus it is simply y=a(x)+b(x)+a*b(x). There is an assumption that TV spending & sales gained do directly affect the online spending on ads & sales gained from online spending (and vice versa).\n",
    "\n",
    "In the context of a Continuous Variable, the variable inputs (beta^1 and beta^2 and beta^3) would be sales gained for each $1,000 in advertisement spending (for both TV & Online). The output variable would be the amount of sales gained. Beta^0 would be sales without any form of advertisement spending (let's just assume 100 sales). \n",
    "\n",
    "**Binary Variable:**\n",
    "\n",
    "- W/o interaction: $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad Spend}) + \\beta_2 (\\text{Online Ad Spend})$$\n",
    "\n",
    "\n",
    "- With interaction: $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad Spend}) + \\beta_2 (\\text{Online Ad Spend}) + \\beta_3 (\\text{TV Ad Spend} \\times \\text{Online Ad Spend})$$\n",
    "\n",
    "The assumption here is D=1 if spending is over 1,000 (high), and D=0 is under 1,000 (low)\n",
    "\n",
    "TV_ad_spent = sales * (0, 1)\n",
    "\n",
    "Online_ad_spent = sales * (0, 1)\n",
    "\n",
    "The non-interaction equation assumes TV spending & Online spending are seperate. There is an assumption that TV spending & sales gained does not affect the online spending on ads & sales gained from online spending (and vice versa).\n",
    "\n",
    "The interaction equation assumes TV spending & Online spending are dependent on each other. There is an assumption that TV spending & sales gained do directly affect the online spending on ads & sales gained from online spending (and vice versa).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e39b8e",
   "metadata": {},
   "source": [
    "# CoPilot Summary for Question 2\n",
    "\n",
    "Absolutely! Here's a summary of our conversation:\n",
    "\n",
    "### Summary of Conversation\n",
    "\n",
    "We discussed how to model the relationship between advertising spending (on TV and online platforms) and sales using both non-interaction and interaction models.\n",
    "\n",
    "#### Non-Interaction Model\n",
    "- **Formula:** $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad Spend}) + \\beta_2 (\\text{Online Ad Spend})$$\n",
    "- **Explanation:** This model assumes that the effects of TV ad spend and online ad spend on sales are independent. Each type of ad spend contributes additively to the total sales.\n",
    "\n",
    "#### Interaction Model\n",
    "- **Formula:** $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad Spend}) + \\beta_2 (\\text{Online Ad Spend}) + \\beta_3 (\\text{TV Ad Spend} \\times \\text{Online Ad Spend})$$\n",
    "- **Explanation:** This model includes an interaction term that captures the combined effect of TV and online ad spending. The interaction term (\\(\\beta_3\\)) allows the effect of one type of ad spend to depend on the level of the other type of ad spend.\n",
    "\n",
    "#### Key Differences\n",
    "- **Independence vs. Dependence:** The non-interaction model treats the effects of TV and online ad spends as independent, while the interaction model accounts for the possibility that the effectiveness of one type of ad spend depends on the level of the other.\n",
    "- **Complexity:** The interaction model is more complex because it includes an additional term to capture the combined effects.\n",
    "- **Predictive Power:** The interaction model can provide better predictions if there is a significant interaction between TV and online ad spending.\n",
    "\n",
    "#### Binary Predictor Variables\n",
    "We also discussed how to model the scenario using binary predictor variables (high vs. low ad spending):\n",
    "- **Non-Interaction Model:** $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad High}) + \\beta_2 (\\text{Online Ad High})$$\n",
    "- **Interaction Model:** $$\\text{Sales} = \\beta_0 + \\beta_1 (\\text{TV Ad High}) + \\beta_2 (\\text{Online Ad High}) + \\beta_3 (\\text{TV Ad High} \\times \\text{Online Ad High})$$\n",
    "\n",
    "In both cases, the interaction model captures the combined effect of high spending on both TV and online ads, potentially leading to higher sales than simply adding the individual effects.\n",
    "\n",
    "Feel free to use this summary for your homework assignment! If you need any more details or clarifications, just let me know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf901a6",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "\n",
    "The statement \"the model only explains 17.6% of the variability in the data\" implies that the R^2 value is 0.176, meaning that most of the variance (82.4%) from the outcome variable is not explained by the predictor variable. A low R^2 could indicate a very weak relationship between the predictor and outcome variable.\n",
    "\n",
    "The statement \"while at the same time \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\" implies that the coefficients are large, thus there is a strong predictor and outcome relationship, and strong evidence against the null hypothesis of \"no effect\" further implies that there is a strong correlation between the predictor and outcome variables\n",
    "\n",
    "This can all be true at the same time with no contradictions. A predictor have a strong impact on the outcome but not on the variability if there are significant other factors that also influence the variability. This is partially due to the model being flawed. The variability may be low due to other factors, however if the other factors are not included as predictor variables, the predictor in the model can artificially have a high coefficient due to correlation, especially if the model does not capture the complex nuance of the outcome variable.\n",
    "\n",
    "The p-value indicates that the predictor has a significant effect, however, there is no contradiction with this in regards to R^2 for similar reasons. The Sp. Def and Generation are statistically significant in a hypothesis test on affecting HP, however in regards to variability, there can be major other factors that are not taken into account for. In a literal sense, the variability is not strongly influenced by Sp. Def and Generation, but they are good indicators of HP change within the context of the model due to the model not taking other factors into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042c4506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Fri, 15 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:27:43</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Fri, 15 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     02:27:43     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Fri, 15 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        02:27:43   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64aebc49",
   "metadata": {},
   "source": [
    "# CoPilot Summary (Q4)\n",
    "\n",
    "Sure, here's a summary of our conversation:\n",
    "\n",
    "### Summary of Our Conversation\n",
    "\n",
    "1. **Low R-squared Value**:\n",
    "   - The R-squared value of 0.176 indicates that the model explains only 17.6% of the variability in HP. This suggests that most of the variability (82.4%) is due to other factors not included in the model.\n",
    "\n",
    "2. **Significant Predictors**:\n",
    "   - Despite the low R-squared value, the predictors (Sp. Def and Generation) have large coefficients and low p-values. This means they are statistically significant and have a strong impact on HP within the context of the model.\n",
    "\n",
    "3. **Reconciling Low R-squared and Low P-values**:\n",
    "   - A low R-squared value indicates that the model does not capture much of the overall variability in HP, suggesting the presence of other influencing factors. However, the low p-values indicate that Sp. Def and Generation are significant predictors, meaning their relationship with HP is meaningful and not due to random chance.\n",
    "\n",
    "4. **Correlation vs. Causation**:\n",
    "   - The coefficients indicate a strong correlation between the predictors and HP, but this does not necessarily imply causation. Establishing causation requires additional evidence beyond the regression model.\n",
    "\n",
    "5. **Model Limitations**:\n",
    "   - The model might be incomplete, missing other important factors that influence HP. This can lead to a low R-squared value, even if the included predictors are significant.\n",
    "\n",
    "6. **Key Takeaway**:\n",
    "   - Sp. Def and Generation are good indicators of HP change within the context of the model, but the low R-squared value suggests that there are other significant factors influencing HP that are not accounted for.\n",
    "\n",
    "### Final Statement\n",
    "\n",
    "\"In a literal sense, the variability in HP is not strongly influenced by Sp. Def and Generation alone, as indicated by the low R-squared value. However, within the context of the model, Sp. Def and Generation are good indicators of HP change, as they are statistically significant predictors. This significance suggests that their relationship with HP is meaningful, even though the model does not account for other factors that also influence HP.\"\n",
    "\n",
    "Feel free to use this summary for your homework assignment. If you need any more details or further clarification, just let me know!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2606399",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "\n",
    "The code splits data on pokemon into two groups evenly, a training set (used to create linear regression models) and a testing set (used to evaluate how reliable the linear regression models are). \n",
    "\n",
    "With the training set, a Linear Regression model (model 3) is made where the predictor variables are \"Attack\" and \"Defense\" and the outcome variable is \"HP\". \n",
    "\n",
    "With the training set, another Linear Regression Model (model 4) is made where the predictor variables are \"Attack\", \"Defense\", \"Speed\", \"Legendary\", \"Sp. Def\", and \"Sp. Atk\" and the outcome variable is \"HP\". \n",
    "\n",
    "HP Predictions with both models were made on the training set and testing set and then were compared with the actual HP values of their own sets. The correlation coefficient was calculated and then was squared to get in-sample (training set) and out of sample (testing set) R^2. This was intended to measure how reliable the model was towards predictions on data the models had not dealt with. A higher in-sample R^2 compared to out-of-sample R^2 indicates that the model can accurately generalize data outside of the data used to fit the linear regression models. If it fails to do this, the models are \"overfit\" and include too much complexity.\n",
    "\n",
    "\n",
    "**Model 3: Multiple Linear Regression**\n",
    "\n",
    "Formula: HP ~ Attack + Defense\n",
    "\n",
    "In-sample R-squared: 0.14771558304519894\n",
    "\n",
    "Out-of-sample R-squared: 0.21208501873920738\n",
    "\n",
    "\n",
    "\n",
    "**Model 4: Complex Linear Regression with Interaction Terms**\n",
    "\n",
    "Formula: HP ~ Attack * Defense * Speed * Legendary * Q(\"Sp. Def\") * Q(\"Sp. Atk\")\n",
    "\n",
    "In-sample R-squared: 0.46709442115833855\n",
    "\n",
    "Out-of-sample R-squared: 0.002485342598992873\n",
    "\n",
    "\n",
    "Model 4 has a larger in-sample R^2 than out of sample R^2 (and Model 3 does not) thus is too overfit & complex for generalizations of outside data. Model 3 has the reverse thus can be used as a model to generalize outside data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a7f5bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n",
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()\n",
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)\n",
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()\n",
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54903072",
   "metadata": {},
   "source": [
    "# CoPilot Summary (Q5)\n",
    "\n",
    "Of course! Here's a summary of our discussion:\n",
    "\n",
    "### Summary of Our Discussion\n",
    "\n",
    "1. **Data Preparation**:\n",
    "   - **Handling Missing Values**: Replaced `NaN` values in the \"Type 2\" column with the string `'None'`.\n",
    "   - **Splitting the Dataset**: Divided the dataset into training and testing sets with a 50/50 split, ensuring reproducibility by setting a random seed.\n",
    "\n",
    "2. **Model 3: Multiple Linear Regression**:\n",
    "   - **Input Variables**: `Attack` and `Defense`.\n",
    "   - **Output Variable**: `HP`.\n",
    "   - **Model Specification**: Used an OLS regression model to predict `HP` based on `Attack` and `Defense`.\n",
    "   - **Model Evaluation**: Calculated 'in sample' R-squared on the training set and 'out of sample' R-squared on the testing set to assess the model's performance.\n",
    "\n",
    "3. **Model 4: Complex Linear Regression with Interaction Terms**:\n",
    "   - **Input Variables**: `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, `Sp. Atk`, and their interactions.\n",
    "   - **Output Variable**: `HP`.\n",
    "   - **Model Specification**: Used an OLS regression model with interaction terms to predict `HP`.\n",
    "   - **Model Evaluation**: Calculated 'in sample' R-squared on the training set and 'out of sample' R-squared on the testing set to assess the model's performance.\n",
    "\n",
    "4. **Testing Set**:\n",
    "   - **Purpose**: Used to evaluate the model's performance and ensure its reliability by assessing how well it generalizes to new, unseen data.\n",
    "   - **Evaluation Metrics**: Calculated the 'out of sample' R-squared value by comparing the actual `HP` values to the predicted `HP` values on the testing set.\n",
    "\n",
    "5. **Correlation Coefficient and R-squared**:\n",
    "   - **Correlation Coefficient (\\( r \\))**: Measures the strength and direction of the linear relationship between two variables, ranging from -1 to 1.\n",
    "   - **R-squared (\\( R^2 \\))**: Represents the proportion of the variance in the dependent variable explained by the independent variables. In simple linear regression, \\( R^2 \\) is the square of the correlation coefficient.\n",
    "\n",
    "### Key Points\n",
    "- **Model Training**: The models were trained on the training set.\n",
    "- **Model Evaluation**: The testing set was used to evaluate the models' performance and ensure they generalize well to new data.\n",
    "- **R-squared**: Used as a key metric to assess the models' fit and predictive power.\n",
    "\n",
    "Feel free to include this summary in your homework assignment. If you need any more details or further clarification, just let me know! Good luck with your assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0aca85",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "\n",
    "Model 3 includes the predictor variables of \"Attack\" and \"Defense\" to generate the outcome variable of \"HP\" in the regression model. Model 4 builds upon this by including additional predictor variables of \"Speed\", \"Legendary\", \"Sp. Def\", and \"Sp. Atk\" alongside \"Attack\" and \"Defense\" to generate the outcome variable of \"HP\". However, it was too complex, lacked nuance, and had a poor out-of-sample R^2 difference of 0.44. Model 5 refines this by splitting up the data into 3 categorical indicators of \"Generation\", \"Type 1\" and \"Type 2\" to make the model analysis more nuanced, and easier and more reliable to analyze. However, it still faced the same issue of Model 4, in which the out-of-sample R^2 was smaller than the in-sample R^2. However, it was not as bad. Regardless, it was still overfit and for generalizing data outside of the model:\n",
    "\n",
    "**Model 5 R^2**\n",
    "\n",
    "'In sample' R-squared:     0.3920134083531893\n",
    "\n",
    "'Out of sample' R-squared: 0.30015614488652215\n",
    "\n",
    "\n",
    "Model 6 builds upon Model 5 by splitting the indicators of \"Generation\" into \"Generation 2\" and \"Generation 5\" and splitting \"Type 1\" into \"Normal\" and \"Water\". Furthermore it removed \"Defense\", \"Type 2\", and \"Legendary\" as predictor variables. Lastly, it focused primarily on Attack, Speed, Sp. Def, and Sp. Atk. The out-of-sample R^2 was still larger than the in-sample, however by a significantly lesser margin (0.03 compared to 0.09 in Model 5).\n",
    "\n",
    "**Model 6 R^2**\n",
    "\n",
    "'In sample' R-squared:     0.3326310334310908\n",
    "\n",
    "'Out of sample' R-squared: 0.29572460427079933\n",
    "\n",
    "\n",
    "\n",
    "Model 7 builds upon Model 6 by keeping the same categorical indicators but also including linear interactions between the predictor variables of Attack, Speed, Sp. Def, and Sp. Atk to analyze combinations of these predictor variables on the HP. This made the model significantly more nuanced among the relationships between the predictor variables and the outcome. As a result, the R^2 difference between the out-of-sample and in-sample is only around 0.02, an improvement from 0.03.\n",
    "\n",
    "**Model 7 R^2**\n",
    "\n",
    "'In sample' R-squared:     0.37818209127432456\n",
    "\n",
    "'Out of sample' R-squared: 0.35055389205977444\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41172ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n",
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n",
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "\n",
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()\n",
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)\n",
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()\n",
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)\n",
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()\n",
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)\n",
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling\n",
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9719736",
   "metadata": {},
   "source": [
    "# CoPilot Summary (Q7)\n",
    "\n",
    "Sure! Here's a summary of our conversation:\n",
    "\n",
    "### Summary of Model Development and Extension\n",
    "\n",
    "#### From Model 3 to Model 5\n",
    "- **Model 3:** \n",
    "  - **Formula:** `HP ~ Attack + Defense`\n",
    "  - **Rationale:** Basic linear regression using `Attack` and `Defense` as predictors for `HP`.\n",
    "\n",
    "- **Model 4:** \n",
    "  - **Formula:** `HP ~ Attack * Defense * Speed * Legendary * Q(\"Sp. Def\") * Q(\"Sp. Atk\")`\n",
    "  - **Rationale:** Expanded model with multiple interaction terms to capture complex relationships between predictors and `HP`.\n",
    "\n",
    "- **Model 5:** \n",
    "  - **Formula:** `HP ~ Attack + Defense + Speed + Legendary + Q(\"Sp. Def\") + Q(\"Sp. Atk\") + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))`\n",
    "  - **Rationale:** Refined model including categorical variables (`Generation`, `Type 1`, `Type 2`) to account for non-numeric influences and simplify interaction terms.\n",
    "\n",
    "#### From Model 5 to Model 6\n",
    "- **Model 6:** \n",
    "  - **Formula:** `HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\") + I(Q(\"Type 1\")==\"Normal\") + I(Q(\"Type 1\")==\"Water\") + I(Generation==2) + I(Generation==5)`\n",
    "  - **Rationale:** Focused on significant predictors (`Attack`, `Speed`, `Sp. Def`, `Sp. Atk`) and included specific indicators for `Type 1` (`Normal`, `Water`) and `Generation` (`2`, `5`). Removed less significant predictors (`Defense`, `Legendary`, `Type 2`).\n",
    "\n",
    "#### From Model 6 to Model 7\n",
    "- **Model 7:** \n",
    "  - **Formula:** `HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\") + I(Q(\"Type 1\")==\"Normal\") + I(Q(\"Type 1\")==\"Water\") + I(Generation==2) + I(Generation==5)`\n",
    "  - **Rationale:** Introduced interaction terms between `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk` to capture combined effects. Continued use of significant indicators from Model 6.\n",
    "\n",
    "### Key Points\n",
    "- **Model 3:** Basic model with `Attack` and `Defense`.\n",
    "- **Model 4:** Expanded model with multiple interaction terms.\n",
    "- **Model 5:** Refined model with categorical variables.\n",
    "- **Model 6:** Focused on significant predictors and specific indicators.\n",
    "- **Model 7:** Introduced interaction terms for combined effects.\n",
    "\n",
    "### In-Sample and Out-of-Sample \\( R^2 \\) for Model 5\n",
    "- **In-Sample \\( R^2 \\):** 0.4522\n",
    "- **Out-of-Sample \\( R^2 \\):** 0.0201\n",
    "\n",
    "### Interaction Terms\n",
    "- **Purpose:** Capture combined effects of predictors on `HP`.\n",
    "- **Example:** `Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")` includes main effects and all possible interactions between these variables.\n",
    "\n",
    "This summary encapsulates the development and extension of the models, highlighting the rationale behind each step and the key changes made. If you need any further details or have additional questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b074106a",
   "metadata": {},
   "source": [
    "# Question 9 \n",
    "\n",
    "The data below uses the code on Models 6 and 7 to create two identical models (for both Models 6 & 7 respectively) where the training set is exclusively Generation 1 Pokemon and Generation 1-5 Pokemon. \n",
    "\n",
    "Afterwards, the In-Sample and Out-Of-Sample R^2 values (for both models on versions 6 and 7) are calculated and highlighted, and compared alongside the original In-Sample and Out-Of-Sample R^2 values for the original Models 6 and 7.\n",
    "\n",
    "**MODEL 6**\n",
    "\n",
    "'In sample' R-squared:     0.3326310334310908 (original)\n",
    "\n",
    "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
    "\n",
    "\n",
    "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
    "\n",
    "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n",
    "\n",
    "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
    "\n",
    "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n",
    "\n",
    "**MODEL 7**\n",
    "\n",
    "'In sample' R-squared:     0.37818209127432456 (original)\n",
    "\n",
    "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
    "\n",
    "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
    "\n",
    "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n",
    "\n",
    "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
    "\n",
    "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n",
    "\n",
    "We can see that for Generation 1 testing sets on Models 6 and 7, the In-Sample R^2 is extremely high yet the R^2 is extremely low on Out-Of-Sample. This is also seen to a lesser extent on the Generation 1-5 predictions. Although Models 6 and 7 had decent out-of-sample generalizability in Question 7 due to the nuanced structure of the model, this question exposes how poorly this backfires for out-of-sample generalizations if the testing set (which the model is based upon) is extremely poor and narrow. The nuance of the model and the narrow testing set (e.g Generation 1 Pokemon) allow for a high in-sample R^2, however, it cannot perform well on out-of-sample data due to the data the model is based upon being flawed. Models 6 and 7 built upon the testing set of Generations 1-5 have the exact same flaw, but to a lesser extent, since the Pokemon that can be selected is not as narrow and thus the data isn't as egregiously flawed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac543068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL 7 BELOW\n",
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n",
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.35055389205977444 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n",
      "MODEL 6 Below\n",
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n",
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.29572460427079933 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train\n",
    "print(\"MODEL 7 BELOW\")\n",
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")\n",
    "print(\"MODEL 6 Below\")\n",
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")\n",
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d77752e",
   "metadata": {},
   "source": [
    "# CoPilot Summary (Q9)\n",
    "\n",
    "Of course! Here's a summary of our conversation after the first summary:\n",
    "\n",
    "### Interaction Terms in Model 7\n",
    "- **Explanation:** Interaction terms in Model 7 capture the combined effects of predictors like `Attack`, `Speed`, `Sp. Def`, and `Sp. Atk` on `HP`. This means the model considers not just the individual effects but also how these variables interact with each other.\n",
    "\n",
    "### In-Sample and Out-of-Sample \\( R^2 \\) for Model 5\n",
    "- **In-Sample \\( R^2 \\):** 0.4522\n",
    "- **Out-of-Sample \\( R^2 \\):** 0.0201\n",
    "- **Interpretation:** A higher in-sample \\( R^2 \\) compared to out-of-sample \\( R^2 \\) suggests potential overfitting, where the model performs well on training data but poorly on new data.\n",
    "\n",
    "### Performance of Models 6 and 7\n",
    "- **Poor Out-of-Sample Performance:** Models 6 and 7 perform poorly out-of-sample when tested on Generation 1 and Generations 1-5.\n",
    "- **Reasons:**\n",
    "  - **Overfitting:** Models might be overfitting to the specific patterns in the training data.\n",
    "  - **Differences in Characteristics:** Different generations might have distinct characteristics that the models fail to generalize.\n",
    "  - **Model Complexity:** Interaction terms add complexity, which might not generalize well to subsets with different interaction patterns.\n",
    "  - **Sample Size and Variability:** Limited data and high variability within specific generations can affect model performance.\n",
    "  - **Model Assumptions:** Assumptions made by the models might not hold true for specific generations.\n",
    "\n",
    "This summary encapsulates the key points discussed after the first summary. If you need any further details or have additional questions, feel free to ask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f11b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
